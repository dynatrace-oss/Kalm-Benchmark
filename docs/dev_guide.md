<!-- markdownlint-disable MD040 -->
# Dev Guide

## 1) Overview

Directory Structure

```
.
├── data                                # directory with scanner results
│
├── kalm_benchmark                      # source code of the package
│   ├── cli.py                          # Command line interface
│   ├── evaluation                      # module for evaluation the scanner results
│   │   ├── scanner                     # module containing dedicated scripts per scanner for parsing its results
│   │   │   ├── ...
│   │   │   └── scanner_evaluator.py    # the base for the scanner specific scripts
│   │   ├── ccss/                       # CCSS functionality module
│   │   │   ├── ccss_models.py          # CCSS data models and structures
│   │   │   ├── ccss_database.py        # CCSS database operations
│   │   │   ├── ccss_service.py         # high-level CCSS service interface
│   │   │   └── ccss_converter.py       # converter between existing and CCSS formats
│   │   ├── database.py                 # unified SQLite database for all KALM data
│   │   ├── service.py                  # unified service interface replacing file-based storage
│   │   ├── evaluation.py               # the core of the evaluation module
│   │   ├── scanner_manager.py          # scanner discovery and management
│   │   ├── file_index.py               # manifest file indexing
│   │   └── utils.py                    # evaluation utility functions
│   ├── manifest_generator              # module for generating the benchmark manifests
│   │   ├── cdk8s_imports               # (generated) imports of k8s definitions generated by cdk8s
│   │   ├── constants.py                # collection of constants shared across manifests
│   │   ├── gen_manifests.py            # entry-point for generating the manifests
│   │   ├── ...
│   │   └── workload                    # definition of workload related manifests
│   └── ui                              # module for the visualization of the evaluation
│       ├── _pages                      # UI page modules (overview, scanner details, CCSS)
│       │   ├── overview.py             # main overview page
│       │   ├── scanner_details.py      # individual scanner analysis page
│       │   └── ccss_overview.py        # CCSS alignment analysis page
│       ├── scanner_details             # scanner-specific UI components
│       ├── app.py                      # main Streamlit application entry point
│       ├── constants.py                # UI constants and session keys
│       ├── logging_config.py           # centralized logging configuration
│       └── utils.py                    # UI utility functions
│
├── manifests                                # (generated) the target directory for generated manifests
├── notebooks                           # folder containing all notebooks used for the analysis
│── tests                               # all unit-tests mirroring source code structure
└── tox.ini                             # tox file with settings for flake8 and running tox
```

The project consists of 3 main modules:

- `manifest_generator`: the code for the generation and management of the manifests for the benchmark
- `evaluation`: contains all the code for scanners and their evaluations, including CCSS integration
- `ui`: the web interface for visualization and analysis, including CCSS alignment views

### CCSS Module Components

The evaluation module includes CCSS (Common Configuration Scoring System) functionality and unified data storage:

- `database.py`: Unified SQLite database for all scanner results and CCSS data
- `service.py`: High-level service interface replacing file-based storage
- `ccss/ccss_models.py`: Data models for CCSS findings and alignment analysis
- `ccss/ccss_database.py`: CCSS-specific database operations (integrated with unified DB)
- `ccss/ccss_service.py`: High-level service interface for CCSS operations
- `ccss/ccss_converter.py`: Conversion utilities between existing and CCSS data formats

### Unified Database Storage  

The benchmark uses a unified SQLite database instead of file-based storage:

- **Performance**: Database queries replace file system operations
- **CCSS Integration**: Native support for CCSS scoring and alignment analysis
- **Unified Storage**: All scanner results, evaluations, and CCSS data in one database
- **Historical Tracking**: Complete audit trail of all scan runs and evaluations  

## Setup

### 1) Installation

To install the project and all the dependencies execute:

```shell
poetry run install
```

After the installation the pre-commit hooks must be installed:

```shell
poetry run pre-commit install
```

This installs the following tools with minor adjustments:

- [black](https://black.readthedocs.io/en/stable/) to format the code
- [flake8](https://flake8.pycqa.org/en/latest/) to lint the code
- [isort](https://pycqa.github.io/isort/) to sort the import statements

### 2) Dependency management

[Poetry](https://python-poetry.org) is used for the management of this project.
Thus, to install new dependency also use poetry instead of pip:

```shell
poetry add <dependency>
```

### 3) Run Tests

For unit-tests [pytest](https://docs.pytest.org/en/) is used. To run all tests enter:

```shell
poetry run pytest
```

## Evaluation pipeline

The evaluation of the scanner results run through a unified pipeline.
At the beginning the results of a scanner are loaded and parsed. These steps are specified to every scanner and can be customized in the respective implementation.
Afterwardse, the results are structured as a table and any missing check ids are imputed.
In parallel, the benchmark table is created by implicetely generating the manifests using [cdk8s](https://cdk8s.io/docs/latest/).

Both tables are merged on their `check_id` column and the respective column containing information of the `checked path`.
Finally, the outcome is post-processed by removing duplicates and missing values and categorizing the checks.

The full pipeline is as follows:

```mermaid
flowchart TD;
    subgraph scanner
        direction LR;
        A[Load Scan Result] --> scan_parse[Parse Results JSON]
        scan_parse --> impute[Impute Check ID]
        impute --> tab_scan[Tabulate Results]
    end
tab_scan --> merge{Merge <br/> dataframes}
    subgraph benchmark
        B[Load Benchmark] --> tab_bench[Tabulate Benchmark]
    end
tab_bench --> merge
merge --> drop_redundants[Drop Redundant Checks]
drop_redundants --> drop_dups[Drop Duplicates]
drop_dups --> single_name[Unify Names]
single_name --> cat[Categorize Check]
cat --> oos[Filter Out of Scope Results]
oos --> fill_na[Fill NAs]
fill_na --> classify[Classify Benchmark Result]
classify --> Z([Done])
```
